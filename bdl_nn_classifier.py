# -*- coding: utf-8 -*-
"""Copia de bdl_NicoleKiedanski.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fn9aLAY1XjhTTL5XoS6FzjD4REh9ZSZs

Configuracion y chequeo del entorno

Conectar con Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""Instalacion de modulos y bibliotecas"""

import tensorflow as tf
print("Tensorflow version " + tf.__version__)
import tensorflow_probability as tfp
import os
import cv2
import zipfile
import numpy as np
from sklearn.model_selection import train_test_split

from tensorflow import keras
from tensorflow.keras import layers

"""Armado de la red neuronal - convolucional"""

#preprocesamiento de las imagenes // se debe cambiar la carpeta!
trainpath_source = "/content/drive/MyDrive/Master/Bioinformatica/BDL/CatsAndDogs.zip"
files_path = "/content"
trainpath = "/content/train"

with zipfile.ZipFile(trainpath_source, 'r') as zipp:
  zipp.extractall(files_path)

!ls /content/train | wc -l

images = []
labels = []

for image in os.scandir(trainpath):
  img = cv2.imread(trainpath+"/"+image.name)           #leo las imagenes desde la carpeta
  img_mtx = cv2.resize(img, (180,180))            #transformo a mismo tama√±o, classe narray
  images.append(list(img_mtx))
  if image.name.startswith("cat"):
    lebel = 0
  elif image.name.startswith("dog"):
    lebel = 1
  labels.append(lebel)

#images = tf.data.Dataset.from_tensor_slices(images)

X = np.array(images)
Y = np.array(labels)

#separo las imagenes en train y testeo
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
from sklearn.utils import shuffle
x_train, y_train = shuffle(x_train, y_train, random_state=True)

#convertimos a one hot encoding
y_test_he = tf.one_hot(y_test,2)
y_train_he = tf.one_hot(y_train,2)

# eliminamos de memoria lo que ya no se va a usar
del images, labels, X, Y, y_train, y_test

#modificacion de las imagenes de forma aleatoria para generar mayor diversidad y evitar baias y armado de la estructura neuronal
model = tf.keras.Sequential([
                 keras.layers.RandomFlip('horizontal',input_shape=(180, 180, 3)), #rota imagen
                 keras.layers.RandomRotation(0.15),                               #rota imagen
                 keras.layers.RandomZoom(0.2),                                    #zoom imagen
                 keras.layers.RandomWidth(0.1),                                   #ensancha imagen
                 keras.layers.RandomHeight(0.1),                                  #alarga imagen
])
model.add(layers.Conv2D(32, (5,5), strides=(2,2), padding="same", activation="relu", input_shape=(180,180,3)))
model.add(layers.MaxPool2D(pool_size=(2,2), strides=(1,1)))
model.add(layers.Conv2D(48, (3,3), strides=(2,2), padding="same", activation="relu"))
model.add(layers.MaxPool2D(pool_size=(2,2), strides=(1,1)))
model.add(layers.Conv2D(64, (3,3), strides=(1,1)))
model.add(layers.GlobalMaxPooling2D())                                            #es como el flatten(), genera un vector
model.add(layers.Dense(256, activation="relu"))
model.add(layers.Dropout(0.3))                                                    #para quitar overfitting
model.add(layers.Dense(256, activation="relu"))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(2))                                                        #la salida tiene 2 neuronas porque me dice si es perro o gato

print(model.summary())

#para visualizar modelo de forma esquematica
from keras.utils.vis_utils import plot_model
plot_model(model, show_shapes=True, show_layer_names=True)

#se define la funcion de perdida, de optimizacion y metricas
model.compile(
    optimizer = "adam",
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), #CategoricalCrossEntropy porque hay mas de una etiqueta y es categorica. Logits=TRUE porque no se uso la funcion de activacion softmax en la capa de salida
    metrics = ["accuracy"],
)

# se definen los callbacks: early stopper y reduce learning rate

early_stopping = keras.callbacks.EarlyStopping(
    patience = 5,
    min_delta = 0.001,
)

learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001,
)

# se ajusta el modelo
history = model.fit(
    x_train, y_train_he,
    validation_data=(x_test, y_test_he),
    batch_size = 250,
    epochs = 30,
    callbacks = [early_stopping, learning_rate_reduction]
)

# se evalua el modelo
import pandas as pd
history_df = pd.DataFrame(history.history)
history_df[["accuracy", "val_accuracy"]].plot()
